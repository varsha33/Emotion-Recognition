{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "multilabel_kea_colab_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipkiVoYv29y"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56689Kbtv4aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28aba630-06c1-45b4-d3cb-0c074f03cc7c"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IozsiDVvwVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c490f991-2f50-475e-e9e8-47c2f09d1359"
      },
      "source": [
        "pip install transformers==3.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buPM1PEEw0M1"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/MyDrive/emotion_recognition\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5nrmB-dP5-e"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "Insert the required parameters in the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5KsX8_v1B3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b42cb1f3-c2e2-487c-a186-956342548c1c"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "## torch packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "## for visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## custom\n",
        "from eval_multilabel import eval_model\n",
        "from select_model_input import select_model,select_input\n",
        "import dataset\n",
        "import config_multilabel as train_config\n",
        "from label_dict import label_emo_map\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best,filename='checkpoint.pth.tar'):\n",
        "\ttorch.save(state,filename)\n",
        "\tif is_best:\n",
        "\t\tshutil.copyfile(filename,filename.replace('checkpoint.pth.tar','model_best.pth.tar'))\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "\tparams = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "\tfor p in params:\n",
        "\t\tp.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iter, epoch,loss_fn,optimizer,config):\n",
        "\n",
        "\ttotal_epoch_loss = 0\n",
        "\ttotal_epoch_acc = 0\n",
        "\tmodel.cuda()\n",
        "\tsteps = 0\n",
        "\tmodel.train()\n",
        "\tstart_train_time = time.time()\n",
        "\tfor idx, batch in enumerate(train_iter):\n",
        "\n",
        "\t\ttext, attn, target = select_input(batch,config,arch_name)\n",
        "\n",
        "\t\tif (len(target)is not config.batch_size):# Last batch may have length different than config.batch_size\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tif arch_name ==\"electra\" or arch_name == \"bert\":\n",
        "\t\t\t\ttext = text.cuda()\n",
        "\t\t\telse:\n",
        "\t\t\t\ttext = [text[0].cuda(),text[1].cuda(),text[2].cuda(),text[3].cuda()]\n",
        "\n",
        "\t\t\tattn = attn.cuda()\n",
        "\t\t\ttarget = target.cuda()\n",
        "\n",
        "\t\t## model prediction\n",
        "\t\tmodel.zero_grad()\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\t# print(\"Prediction\")\n",
        "\t\tprediction = model(text,attn)\n",
        "\t\t# print(\"computing loss\")\n",
        "\t\tloss = loss_fn(prediction, target)\n",
        "\n",
        "\t\t# print(\"Loss backward\")\n",
        "\t\tstartloss = time.time()\n",
        "\t\tloss.backward()\n",
        "\t\t# print(time.time()-startloss,\"Finish loss\")\n",
        "\t\tclip_gradient(model, 1e-1)\n",
        "\t\t# torch.nn.utils.clip_grad_norm_(model.parameters(),1)\n",
        "\t\toptimizer.step()\n",
        "\t\t# print(\"=====================\")\n",
        "\t\tsteps += 1\n",
        "\t\tif steps % 100 == 0:\n",
        "\t\t\tprint (f'Epoch: {epoch+1:02}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Time taken: {((time.time()-start_train_time)/60): .2f} min')\n",
        "\t\t\tstart_train_time = time.time()\n",
        "\n",
        "\t\ttotal_epoch_loss += loss.item()\n",
        "\n",
        "\t\t# break\n",
        "\n",
        "\treturn total_epoch_loss/len(train_iter)\n",
        "\n",
        "def train_model(config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home):\n",
        "\n",
        "\tbest_f1_score = 0\n",
        "\tpatience_flag = 0\n",
        "\ttrain_iter,valid_iter,test_iter = data[0],data[1],data[2] # data is a tuple of three iterators\n",
        "\n",
        "\t# print(\"Start Training\")\n",
        "\tfor epoch in range(config.start_epoch,config.nepoch):\n",
        "\n",
        "\t\t## train and validation\n",
        "\n",
        "\t\ttrain_loss = train_epoch(model, train_iter, epoch,loss_fn,optimizer,config)\n",
        "\n",
        "\t\tval_loss, val_result = eval_model(model, valid_iter,loss_fn,config,arch_name,save_home)\n",
        "\t\tprint(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val. Loss: {val_loss:3f}, Val. F1: {val_result[\"f1\"]:.2f}')\n",
        "\t\t## testing\n",
        "\t\ttest_loss, test_result = eval_model(model, test_iter,loss_fn,config,arch_name,save_home)\n",
        "\n",
        "\t\tprint(f'Test Loss: {test_loss:.3f}, Test F1 score: {test_result[\"f1\"]:.4f}')\n",
        "\n",
        "\t\t## save best model\n",
        "\t\tis_best = val_result[\"f1\"] > best_f1_score\n",
        "\n",
        "\t\t# save_checkpoint({'epoch': epoch + 1,'arch': arch_name,'state_dict': model.state_dict(),'train_acc':train_acc,\"val_acc\":val_acc,'param':log_dict[\"param\"],'optimizer' : optimizer.state_dict(),},is_best,save_home+\"/checkpoint.pth.tar\")\n",
        "\n",
        "\t\tbest_f1_score = max(val_result[\"f1\"], best_f1_score)\n",
        "\t\tif config.step_size != None:\n",
        "\t\t\tlr_scheduler.step()\n",
        "\n",
        "\t\t## save logs\n",
        "\t\tif is_best:\n",
        "\t\t\tprint(\"X\")\n",
        "\n",
        "\t\t\tpatience_flag = 0\n",
        "\n",
        "\t\t\tlog_dict[\"test_result\"] = test_result\n",
        "\t\t\tlog_dict[\"valid_result\"] = val_result\n",
        "\n",
        "\t\t\tlog_dict[\"train_loss\"] = train_loss\n",
        "\t\t\tlog_dict[\"test_loss\"] = test_loss\n",
        "\t\t\tlog_dict[\"valid_loss\"] = val_loss\n",
        "\n",
        "\t\t\tlog_dict[\"epoch\"] = epoch+1\n",
        "\n",
        "\n",
        "\n",
        "\t\t\twith open(save_home+\"/log.json\", 'w') as fp:\n",
        "\t\t\t\tjson.dump(log_dict, fp,indent=4)\n",
        "\t\t\tfp.close()\n",
        "\t\telse:\n",
        "\t\t\tpatience_flag += 1\n",
        "\n",
        "\t\t## early stopping\n",
        "\t\tif patience_flag == config.patience or epoch == config.nepoch-1:\n",
        "\t\t\tprint(log_dict)\n",
        "\t\t\tbreak\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  log_dict = {}\n",
        "  log_dict[\"param\"] = train_config.param\n",
        "  if train_config.tuning:\n",
        "    for arch_name in [\"kea_electra\"]:\n",
        "      for lr in [3e-05]: ## for tuning\n",
        "        np.random.seed(0)\n",
        "        random.seed(0)\n",
        "        torch.manual_seed(0)\n",
        "        torch.cuda.manual_seed(0)\n",
        "        torch.cuda.manual_seed_all(0)\n",
        "        \n",
        "        print('Loading dataset')\n",
        "        start_time = time.time()\n",
        "        vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(train_config.batch_size,train_config.tokenizer,train_config.dataset,arch_name)\n",
        "\n",
        "        data = (train_iter,valid_iter,test_iter)\n",
        "        finish_time = time.time()\n",
        "        print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
        "        learning_rate = lr\n",
        "        arch_name = arch_name\n",
        "        log_dict[\"param\"][\"learning_rate\"] = lr\n",
        "        log_dict[\"param\"][\"arch_name\"] = arch_name\n",
        "        note = \"Tuning learning_rate:\"+str(lr)\n",
        "\n",
        "\t\t\t\t# learning_rate = train_config.learning_rate\n",
        "        input_type = train_config.input_type\n",
        "\n",
        "\t\t\t\t## Initialising model, loss, optimizer, lr_scheduler\n",
        "        model = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "        if train_config.dataset == \"ed\":\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "          loss_fn = nn.BCEWithLogitsLoss()\n",
        "        total_steps = len(train_iter) * train_config.nepoch\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "        if train_config.step_size != None:\n",
        "          lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "\t\t\t\t## Filepaths for saving the model and the tensorboard runs\n",
        "        model_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "        writer = SummaryWriter('/content/gdrive/MyDrive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "        save_home = \"/content/gdrive/MyDrive/emotion_recognition/save/\"+train_config.dataset+\"/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "\t\t\t\t# print(train_config)\n",
        "        train_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n",
        "\telse:\n",
        "\t\tlearning_rate = train_config.learning_rate\n",
        "\n",
        "\t\tarch_name = train_config.arch_name\n",
        "\n",
        "\t\tinput_type = train_config.input_type\n",
        "\n",
        "\t\t## Initialising model, loss, optimizer, lr_scheduler\n",
        "\t\tmodel = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "\n",
        "\t\tif train_config.dataset == \"ed\":\n",
        "\t\t\tloss_fn = nn.CrossEntropyLoss()\n",
        "\t\telse:\n",
        "\t\t\tloss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\t\ttotal_steps = len(train_iter) * train_config.nepoch\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "\n",
        "\n",
        "\t\tlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\t\t## Filepaths for saving the model and the tensorboard runs\n",
        "\t\tmodel_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "\t\twriter = SummaryWriter('/content/gdrive/MyDrive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "\t\tsave_home = \"/content/gdrive/MyDrive/emotion_recognition/save/\"+train_config.dataset+\"/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "\t\ttrain_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset\n",
            "Finished loading. Time taken:01.334 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings_project.weight', 'electra.embeddings_project.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Idx: 100, Training Loss: 0.1808, Time taken:  0.27 min\n",
            "Epoch: 01, Idx: 200, Training Loss: 0.1373, Time taken:  0.28 min\n",
            "Epoch: 01, Idx: 300, Training Loss: 0.1543, Time taken:  0.28 min\n",
            "Epoch: 01, Idx: 400, Training Loss: 0.1332, Time taken:  0.28 min\n",
            "Epoch: 01, Idx: 500, Training Loss: 0.1347, Time taken:  0.28 min\n",
            "Epoch: 01, Idx: 600, Training Loss: 0.1408, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 700, Training Loss: 0.1544, Time taken:  0.31 min\n",
            "Epoch: 01, Idx: 800, Training Loss: 0.1249, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 900, Training Loss: 0.1546, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1000, Training Loss: 0.1443, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1100, Training Loss: 0.1459, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1200, Training Loss: 0.1162, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1300, Training Loss: 0.1557, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1400, Training Loss: 0.1267, Time taken:  0.30 min\n",
            "Epoch: 01, Idx: 1500, Training Loss: 0.1108, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1600, Training Loss: 0.1288, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1700, Training Loss: 0.1043, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 1800, Training Loss: 0.1164, Time taken:  0.30 min\n",
            "Epoch: 01, Idx: 1900, Training Loss: 0.0996, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 2000, Training Loss: 0.0941, Time taken:  0.29 min\n",
            "Epoch: 01, Idx: 2100, Training Loss: 0.1393, Time taken:  0.30 min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 0.144, Val. Loss: 0.111300, Val. F1: 0.11\n",
            "Test Loss: 0.109, Test F1 score: 0.1179\n",
            "X\n",
            "Epoch: 02, Idx: 100, Training Loss: 0.1079, Time taken:  0.28 min\n",
            "Epoch: 02, Idx: 200, Training Loss: 0.1121, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 300, Training Loss: 0.1122, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 400, Training Loss: 0.0942, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 500, Training Loss: 0.1370, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 600, Training Loss: 0.0991, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 700, Training Loss: 0.0865, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 800, Training Loss: 0.1439, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 900, Training Loss: 0.0810, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 1000, Training Loss: 0.1051, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 1100, Training Loss: 0.1207, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 1200, Training Loss: 0.1291, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 1300, Training Loss: 0.1069, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 1400, Training Loss: 0.0957, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 1500, Training Loss: 0.0868, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 1600, Training Loss: 0.0995, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 1700, Training Loss: 0.1159, Time taken:  0.31 min\n",
            "Epoch: 02, Idx: 1800, Training Loss: 0.0919, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 1900, Training Loss: 0.0836, Time taken:  0.29 min\n",
            "Epoch: 02, Idx: 2000, Training Loss: 0.1109, Time taken:  0.30 min\n",
            "Epoch: 02, Idx: 2100, Training Loss: 0.0815, Time taken:  0.29 min\n",
            "Epoch: 02, Train Loss: 0.100, Val. Loss: 0.096458, Val. F1: 0.23\n",
            "Test Loss: 0.095, Test F1 score: 0.2378\n",
            "X\n",
            "Epoch: 03, Idx: 100, Training Loss: 0.0819, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 200, Training Loss: 0.0937, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 300, Training Loss: 0.0783, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 400, Training Loss: 0.0915, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 500, Training Loss: 0.0856, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 600, Training Loss: 0.1048, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 700, Training Loss: 0.0634, Time taken:  0.31 min\n",
            "Epoch: 03, Idx: 800, Training Loss: 0.1088, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 900, Training Loss: 0.0961, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1000, Training Loss: 0.0697, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1100, Training Loss: 0.0867, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1200, Training Loss: 0.0661, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 1300, Training Loss: 0.0976, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 1400, Training Loss: 0.0985, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1500, Training Loss: 0.0603, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 1600, Training Loss: 0.1168, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 1700, Training Loss: 0.1032, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1800, Training Loss: 0.0664, Time taken:  0.29 min\n",
            "Epoch: 03, Idx: 1900, Training Loss: 0.0938, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 2000, Training Loss: 0.0732, Time taken:  0.30 min\n",
            "Epoch: 03, Idx: 2100, Training Loss: 0.0920, Time taken:  0.29 min\n",
            "Epoch: 03, Train Loss: 0.087, Val. Loss: 0.091995, Val. F1: 0.35\n",
            "Test Loss: 0.090, Test F1 score: 0.3587\n",
            "X\n",
            "Epoch: 04, Idx: 100, Training Loss: 0.0870, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 200, Training Loss: 0.0408, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 300, Training Loss: 0.0631, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 400, Training Loss: 0.0439, Time taken:  0.31 min\n",
            "Epoch: 04, Idx: 500, Training Loss: 0.0556, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 600, Training Loss: 0.0662, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 700, Training Loss: 0.0995, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 800, Training Loss: 0.1043, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 900, Training Loss: 0.0929, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 1000, Training Loss: 0.0817, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 1100, Training Loss: 0.0312, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 1200, Training Loss: 0.0957, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 1300, Training Loss: 0.0759, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 1400, Training Loss: 0.0453, Time taken:  0.30 min\n",
            "Epoch: 04, Idx: 1500, Training Loss: 0.0676, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 1600, Training Loss: 0.1002, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 1700, Training Loss: 0.1021, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 1800, Training Loss: 0.0555, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 1900, Training Loss: 0.0918, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 2000, Training Loss: 0.0876, Time taken:  0.29 min\n",
            "Epoch: 04, Idx: 2100, Training Loss: 0.0558, Time taken:  0.29 min\n",
            "Epoch: 04, Train Loss: 0.079, Val. Loss: 0.089622, Val. F1: 0.38\n",
            "Test Loss: 0.088, Test F1 score: 0.3895\n",
            "X\n",
            "Epoch: 05, Idx: 100, Training Loss: 0.0715, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 200, Training Loss: 0.0562, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 300, Training Loss: 0.0735, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 400, Training Loss: 0.1135, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 500, Training Loss: 0.0838, Time taken:  0.32 min\n",
            "Epoch: 05, Idx: 600, Training Loss: 0.0553, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 700, Training Loss: 0.0785, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 800, Training Loss: 0.0561, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 900, Training Loss: 0.0645, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1000, Training Loss: 0.0874, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 1100, Training Loss: 0.0984, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1200, Training Loss: 0.0945, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 1300, Training Loss: 0.0819, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 1400, Training Loss: 0.1025, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1500, Training Loss: 0.0714, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1600, Training Loss: 0.1050, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1700, Training Loss: 0.0933, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 1800, Training Loss: 0.0812, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 1900, Training Loss: 0.0695, Time taken:  0.29 min\n",
            "Epoch: 05, Idx: 2000, Training Loss: 0.0815, Time taken:  0.30 min\n",
            "Epoch: 05, Idx: 2100, Training Loss: 0.0412, Time taken:  0.29 min\n",
            "Epoch: 05, Train Loss: 0.072, Val. Loss: 0.091782, Val. F1: 0.41\n",
            "Test Loss: 0.091, Test F1 score: 0.4073\n",
            "X\n",
            "Epoch: 06, Idx: 100, Training Loss: 0.0670, Time taken:  0.33 min\n",
            "Epoch: 06, Idx: 200, Training Loss: 0.0610, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 300, Training Loss: 0.0725, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 400, Training Loss: 0.0548, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 500, Training Loss: 0.0811, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 600, Training Loss: 0.0508, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 700, Training Loss: 0.0453, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 800, Training Loss: 0.0619, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 900, Training Loss: 0.0595, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1000, Training Loss: 0.0676, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1100, Training Loss: 0.0675, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1200, Training Loss: 0.0806, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1300, Training Loss: 0.0849, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 1400, Training Loss: 0.0489, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1500, Training Loss: 0.0779, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 1600, Training Loss: 0.0579, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 1700, Training Loss: 0.0412, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 1800, Training Loss: 0.0929, Time taken:  0.30 min\n",
            "Epoch: 06, Idx: 1900, Training Loss: 0.0636, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 2000, Training Loss: 0.0525, Time taken:  0.29 min\n",
            "Epoch: 06, Idx: 2100, Training Loss: 0.0463, Time taken:  0.30 min\n",
            "Epoch: 06, Train Loss: 0.062, Val. Loss: 0.094724, Val. F1: 0.43\n",
            "Test Loss: 0.094, Test F1 score: 0.4283\n",
            "X\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9b998f67965a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                                 \u001b[0;31m# print(train_config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m#       learning_rate = train_config.learning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9b998f67965a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config, data, model, loss_fn, optimizer, lr_scheduler, writer, save_home)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m## train and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0march_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9b998f67965a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iter, epoch, loss_fn, optimizer, config)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# print(\"Prediction\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# print(\"computing loss\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/emotion_recognition/models/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, attn_mask)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IS68MGPXiG"
      },
      "source": [
        "# **Evaluation** \n",
        "\n",
        "Insert model path and log path to run the evaluation"
      ]
    }
  ]
}