{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients,LayerIntegratedGradients,TokenReferenceBase, visualization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from label_dict import label_emo_map\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "\n",
    "class model_wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self,model):\n",
    "        super(model_wrapper, self).__init__()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.model = model\n",
    "    def forward(self, text): #here text is utterance based on the input type specified\n",
    "\n",
    "        output = self.model(text)\n",
    "        return self.softmax(output)\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    return visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            label_emo_map[pred_ind],\n",
    "                            label_emo_map[label],\n",
    "                            label_emo_map[pred_ind],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta)\n",
    "\n",
    "def explain_model(model,binput_ids,btarget,binput_str,bpred_ind,bpred_softmax):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    vis_data_records_ig = []\n",
    "    token_reference = TokenReferenceBase(reference_token_idx=0)\n",
    "    model = model.cpu()\n",
    "    model_explain = model_wrapper(model)\n",
    "    \n",
    "    # print(binput_str)\n",
    "    print(binput_ids[0])\n",
    "#     input_id = binput_ids[i,:].unsqueeze(0).cpu()\n",
    "    seq_len = len(input_id.squeeze(0).tolist())\n",
    "    target = btarget[i].item()\n",
    "    pred_ind  = bpred_ind[i].item()\n",
    "    pred_softmax = bpred_softmax[i,:][pred_ind].item()\n",
    "    input_str = tokenizer.tokenize(\"\".join(binput_str[i]))\n",
    "\n",
    "    if label_emo_map[target] == \"sentimental\" or label_emo_map[target] == \"nostalgic\":\n",
    "        if label_emo_map[pred_ind] == \"nostalgic\" or label_emo_map[pred_ind] == \"sentimental\":\n",
    "            device = torch.device(\"cpu\")\n",
    "            reference_ids = token_reference.generate_reference(seq_len,device=device).unsqueeze(0)\n",
    "            ig =LayerIntegratedGradients(model_explain,model_explain.model.encoder.bert.embeddings)\n",
    "            attributions, delta = ig.attribute(input_id,reference_ids,target=target,n_steps=10,return_convergence_delta=True)\n",
    "\n",
    "            # print('pred: ', label_emo_map[pred_ind], '(', '%.2f'%pred_softmax, ')', ', delta: ', abs(delta))\n",
    "            vis_data_records_ig.append(add_attributions_to_visualizer(attributions,input_str,pred_softmax, pred_ind,target, delta))\n",
    "\n",
    "    if len(vis_data_records_ig) != 0:\n",
    "        visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "import pickle\n",
    "## torch packages\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "## for visulisation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## custom\n",
    "from select_model_input import select_model,select_input\n",
    "import dataset\n",
    "from label_dict import emo_label_map,label_emo_map,class_names,class_indices\n",
    "# from xai_emo_rec import explain_model\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "def get_pred_softmax(logits):\n",
    "    softmax_layer = nn.Softmax(dim=1)\n",
    "    return softmax_layer(logits)\n",
    "\n",
    "def eval_model(model, val_iter, loss_fn,config,arch_name,mode=\"train\",explain=False):\n",
    "\n",
    "    confusion = config.confusion\n",
    "    per_class = config.per_class\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_epoch_acc3 = 0\n",
    "\n",
    "    eval_batch_size = 1\n",
    "\n",
    "    if confusion:\n",
    "        conf_matrix = torch.zeros(config.output_size, config.output_size)\n",
    "    if per_class:\n",
    "           class_correct = list(0. for i in range(config.output_size))\n",
    "           class_total = list(0. for i in range(config.output_size))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            model = model.cuda()\n",
    "            text, attn,target = select_input(batch,config,arch_name)\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                if arch_name==\"a_bert\":\n",
    "                    text = [text[0].cuda(),text[1].cuda()]\n",
    "                    attn = attn.cuda()\n",
    "                elif arch_name == \"va_bert\":\n",
    "                    text = [text[0].cuda(),text[1].cuda(),text[2].cuda()]\n",
    "                    attn = attn.cuda()\n",
    "                elif arch_name == \"vad_bert\" or arch_name==\"kea_bert\" or arch_name == \"self_attn_bert\":\n",
    "                    text = [text[0].cuda(),text[1].cuda(),text[2].cuda(),text[3].cuda()]\n",
    "                    attn = attn.cuda()\n",
    "                else:\n",
    "                    text = text.cuda()\n",
    "                    attn = attn.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            prediction = model(text,attn)\n",
    "\n",
    "            correct = np.squeeze(torch.max(prediction, 1)[1].eq(target.view_as(torch.max(prediction, 1)[1])))\n",
    "            pred_ind = torch.max(prediction, 1)[1].view(target.size()).data\n",
    "\n",
    "            if mode == \"explain\":\n",
    "                pred_softmax = get_pred_softmax(prediction)\n",
    "                explain_model(model,text,target.data,batch[\"utterance_data_str\"],pred_ind,pred_softmax) ## use jupyter-notebook while doing explainations\n",
    "            else:\n",
    "                if confusion:\n",
    "                    for t, p in zip(target.data, pred_ind):\n",
    "                            conf_matrix[t.long(), p.long()] += 1\n",
    "                if per_class:\n",
    "                    label = target[0]\n",
    "                    class_correct[label] += correct.item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "                loss = loss_fn(prediction, target)\n",
    "\n",
    "                num_corrects = (pred_ind == target.data).sum()\n",
    "                y_true.extend(target.data.cpu().tolist())\n",
    "                y_pred.extend(pred_ind.cpu().tolist())\n",
    "\n",
    "                acc = 100.0 * num_corrects/eval_batch_size\n",
    "                acc3 = accuracy_topk(prediction, target, topk=(3,))\n",
    "                total_epoch_loss += loss.item()\n",
    "                total_epoch_acc += acc.item()\n",
    "                total_epoch_acc3 += acc3\n",
    "\n",
    "        if confusion:\n",
    "            import seaborn as sns\n",
    "            sns.heatmap(conf_matrix, annot=True,xticklabels=list(emo_label_map.keys()),yticklabels=list(emo_label_map.keys()),cmap='Blues')\n",
    "\n",
    "            plt.show()\n",
    "        if per_class:\n",
    "            for i in range(config.output_size):\n",
    "                print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                label_emo_map[i], 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "\n",
    "    if mode != \"explain\":\n",
    "\n",
    "        f1_score_e = f1_score(y_true, y_pred, labels=class_indices,average='macro')\n",
    "        f1_score_w = f1_score(y_true, y_pred, labels=class_indices,average='weighted')\n",
    "        return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter),f1_score_e,f1_score_w,total_epoch_acc3/len(val_iter)\n",
    "\n",
    "\n",
    "\n",
    "def load_model(resume,model,optimizer):\n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(resume)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer']) ## during retrain TODO\n",
    "\n",
    "    return model,optimizer,start_epoch\n",
    "    \n",
    "\n",
    "def call_eval(resume_path,mode,rem_epoch=10,patience=10):\n",
    "\n",
    "    ## Load the resume model parameters  \n",
    "    log_path = resume_path.replace(\"model_best.pth.tar\",\"log.json\")\n",
    "    with open(log_path,'r') as f:\n",
    "        log = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    ## Initialising parameters\n",
    "    learning_rate = log[\"param\"][\"learning_rate\"]\n",
    "    batch_size = log[\"param\"][\"batch_size\"]\n",
    "    input_type = log[\"param\"][\"input_type\"]\n",
    "    arch_name = log[\"param\"][\"arch_name\"]\n",
    "    hidden_size = log[\"param\"][\"hidden_size\"]\n",
    "    embedding_length = log[\"param\"][\"embedding_length\"]\n",
    "    output_size = log[\"param\"][\"output_size\"]\n",
    "    tokenizer = log[\"param\"][\"tokenizer\"]\n",
    "    embedding_type = log[\"param\"][\"embedding_type\"]\n",
    "\n",
    "    ## Loading data\n",
    "    print('Loading dataset')\n",
    "    start_time = time.time()\n",
    "    vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(batch_size,tokenizer,embedding_type,arch_name)\n",
    "    finish_time = time.time()\n",
    "    print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
    "\n",
    "    eval_config = edict(log[\"param\"])\n",
    "    eval_config.resume_path = resume_path\n",
    "\n",
    "    if mode == \"explain\":\n",
    "        model = select_model(eval_config,arch_name,vocab_size,word_embeddings,grad_check=False)\n",
    "    else:\n",
    "        model = select_model(eval_config,arch_name,vocab_size,word_embeddings)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,eval_config.step_size, gamma=0.5)\n",
    "\n",
    "    model,optimizer,start_epoch = load_model(resume_path,model,optimizer)\n",
    "\n",
    "        \n",
    "    print(f'Train Acc: {log[\"train_acc\"]:.3f}%, Valid Acc: {log[\"valid_acc\"]:.3f}%, Test Acc: {log[\"test_acc\"]:.3f}%')\n",
    "\n",
    "    eval_config.confusion = False\n",
    "    eval_config.per_class = False\n",
    "\n",
    "    ## explaining\n",
    "    eval_model(model, test_iter,loss_fn,eval_config,arch_name,mode,explain=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below section shows the word importances of sentimental and nostalgic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Finished loading. Time taken:01.817 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 61.888%, Valid Acc: 55.848%, Test Acc: 54.024%\n",
      "[tensor([[  101,  3398,  2055,  2184,  2086,  3283,  1045,  2018,  1037,  7570,\n",
      "         18752, 14116,  3325,  1012,  2009,  2001,  2531,  1003,  2037,  6346,\n",
      "          2021,  2027,  2718,  1996,  2300, 13826,  1998,  5175,  1012,  2027,\n",
      "          2018,  2053,  6441,  2021,  2027,  2471,  2743,  2033,  2125,  1996,\n",
      "          2346,  1012,   102,  2106,  2017,  9015,  2151,  6441,  1029,   102,\n",
      "          2053,  1045,  2347,  1005,  1056,  2718,  1012,  2009,  2357,  2041,\n",
      "          2027,  2020,  7144,  1012,  1045,  2371,  5905,  2021,  3651,  2009,\n",
      "          2001,  2010,  6346,  1012,   102,  2339,  2106,  2017,  2514,  5905,\n",
      "          1029,  2111,  2428,  5807,  1005,  1056,  3298,  7144,  1012,   102,\n",
      "          1045,  2123,  1005,  1056,  2113,  1045,  2001,  2047,  2000,  4439,\n",
      "          1998,  2910,  1005,  1056,  5281,  2505,  2066,  2008,  1012,  1045,\n",
      "          2371,  2066,  2026,  7109,  2081,  2032, 25430,  2121,  3726,  2046,\n",
      "          1996,  2300, 13826,  1012,   102]], device='cuda:0'), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, -0.1700, -0.2200,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.1180,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0580,  0.0000,  0.0000,  0.4310,  0.0000,\n",
      "         -0.3770,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0960,  0.0000,  0.0000,  0.0000,  0.0000,  0.4100,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4310,\n",
      "          0.0000,  0.0000,  0.0380,  0.0000,  0.0000,  0.0000,  0.3000,  0.0000,\n",
      "          0.0000, -0.0830,  0.2700,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0580,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1020,  0.2700,\n",
      "          0.0000, -0.1000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1890,  0.3000,\n",
      "          0.0000,  0.0000,  0.0000, -0.0460,  0.0000,  0.0000, -0.1390,  0.0000,\n",
      "          0.0000, -0.0100,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1130,  0.0000, -0.1470,  0.0000,  0.0000,  0.0000, -0.0830, -0.1470,\n",
      "          0.0000,  0.0100,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.3770,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.1530, -0.1830,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.3270,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.3120,  0.0000,  0.0000, -0.2650,  0.0000,\n",
      "          0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1560,  0.0000,  0.0000,  0.0000,  0.0000, -0.4180,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2650,\n",
      "          0.0000,  0.0000,  0.1300,  0.0000,  0.0000,  0.0000, -0.3060,  0.0000,\n",
      "          0.0000,  0.1430, -0.3650,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3020, -0.3650,\n",
      "          0.0000,  0.1040,  0.0000,  0.0000,  0.0000,  0.0000,  0.1250, -0.3060,\n",
      "          0.0000,  0.0000,  0.0000,  0.1460,  0.0000,  0.0000,  0.2650,  0.0000,\n",
      "          0.0000,  0.4170,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.2710,  0.0000,  0.2190,  0.0000,  0.0000,  0.0000,  0.1430,  0.2190,\n",
      "          0.0000, -0.1150,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.3120,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0080, -0.0960,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.4050,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.3160,  0.0000,  0.0000,  0.1790,  0.0000,\n",
      "         -0.0710,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0960,  0.0000,  0.0000,  0.0000,  0.0000, -0.2410,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1790,\n",
      "          0.0000,  0.0000,  0.0830,  0.0000,  0.0000,  0.0000, -0.2500,  0.0000,\n",
      "          0.0000,  0.0660, -0.1480,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.3160,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0960, -0.1480,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2130, -0.2500,\n",
      "          0.0000,  0.0000,  0.0000,  0.2020,  0.0000,  0.0000,  0.2040,  0.0000,\n",
      "          0.0000,  0.0820,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.4140,  0.0000, -0.0540,  0.0000,  0.0000,  0.0000,  0.0660, -0.0540,\n",
      "          0.0000, -0.1340,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0710,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ecfb587e6f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m call_eval(\"/home/ashvar/varsha/Emotion-Recognition/save/speaker+listener/kea_bert/2020_11_02_15_47_09/model_best.pth.tar\"\n\u001b[0;32m----> 2\u001b[0;31m ,\"explain\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-5e532f563112>\u001b[0m in \u001b[0;36mcall_eval\u001b[0;34m(resume_path, mode, rem_epoch, patience)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m## explaining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0march_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-5e532f563112>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, val_iter, loss_fn, config, arch_name, mode, explain)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"explain\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mpred_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mexplain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"utterance_data_str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_softmax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## use jupyter-notebook while doing explainations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-e8efb7c147a1>\u001b[0m in \u001b[0;36mexplain_model\u001b[0;34m(model, binput_ids, btarget, binput_str, bpred_ind, bpred_softmax)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     input_id = binput_ids[i,:].unsqueeze(0).cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpred_ind\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbpred_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_id' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "call_eval(\"/home/ashvar/varsha/Emotion-Recognition/save/speaker+listener/kea_bert/2020_11_02_15_47_09/model_best.pth.tar\"\n",
    ",\"explain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
