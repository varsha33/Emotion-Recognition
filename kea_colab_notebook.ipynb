{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "kea_colab_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipkiVoYv29y"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56689Kbtv4aL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7838eee3-6a55-4d6b-d756-58b9a2feccaf"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IozsiDVvwVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8854af93-6ff0-4f79-ca5f-241908dce5fc"
      },
      "source": [
        "pip install transformers==3.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buPM1PEEw0M1"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gdrive/MyDrive/emotion_recognition\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5KsX8_v1B3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1808b9a1-ae7f-498d-e60c-9bf13fb0cec2"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "## torch packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "## for visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## custom\n",
        "from eval import eval_model\n",
        "from select_model_input import select_model,select_input\n",
        "import dataset\n",
        "import config as train_config\n",
        "from label_dict import label_emo_map\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best,filename='checkpoint.pth.tar'):\n",
        "\ttorch.save(state,filename)\n",
        "\tif is_best:\n",
        "\t\tshutil.copyfile(filename,filename.replace('checkpoint.pth.tar','model_best.pth.tar'))\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "\tparams = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "\tfor p in params:\n",
        "\t\tp.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iter, epoch,loss_fn,optimizer,config):\n",
        "\n",
        "\ttotal_epoch_loss = 0\n",
        "\ttotal_epoch_acc = 0\n",
        "\tmodel.cuda()\n",
        "\tsteps = 0\n",
        "\tmodel.train()\n",
        "\tstart_train_time = time.time()\n",
        "\tfor idx, batch in enumerate(train_iter):\n",
        "\n",
        "\t\ttext, attn, target = select_input(batch,config,arch_name)\n",
        "\n",
        "\t\ttarget = torch.autograd.Variable(target).long()\n",
        "\n",
        "\t\tif (target.size()[0] is not config.batch_size):# Last batch may have length different than config.batch_size\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tif arch_name ==\"electra\" or arch_name == \"bert\":\n",
        "\t\t\t\ttext = text.cuda()\n",
        "\t\t\telse:\n",
        "\t\t\t\ttext = [text[0].cuda(),text[1].cuda(),text[2].cuda(),text[3].cuda()]\n",
        "\n",
        "\t\t\tattn = attn.cuda()\n",
        "\t\t\ttarget = target.cuda()\n",
        "\n",
        "\n",
        "\t\t## model prediction\n",
        "\t\tmodel.zero_grad()\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\t# print(\"Prediction\")\n",
        "\t\tprediction = model(text,attn)\n",
        "\t\t# print(\"computing loss\")\n",
        "\t\tloss = loss_fn(prediction, target)\n",
        "\n",
        "\t\t## evaluation\n",
        "\t\tnum_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "\t\tacc = 100.0 * num_corrects/config.batch_size\n",
        "\t\t# print(\"Loss backward\")\n",
        "\t\tstartloss = time.time()\n",
        "\t\tloss.backward()\n",
        "\t\t# print(time.time()-startloss,\"Finish loss\")\n",
        "\t\tclip_gradient(model, 1e-1)\n",
        "\t\t# torch.nn.utils.clip_grad_norm_(model.parameters(),1)\n",
        "\t\toptimizer.step()\n",
        "\t\t# print(\"=====================\")\n",
        "\t\tsteps += 1\n",
        "\t\tif steps % 100 == 0:\n",
        "\t\t\tprint (f'Epoch: {epoch+1:02}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%, Time taken: {((time.time()-start_train_time)/60): .2f} min')\n",
        "\t\t\tstart_train_time = time.time()\n",
        "\n",
        "\t\ttotal_epoch_loss += loss.item()\n",
        "\t\ttotal_epoch_acc += acc.item()\n",
        "\n",
        "\treturn total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def train_model(config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home):\n",
        "\n",
        "\tbest_acc1 = 0\n",
        "\tpatience_flag = 0\n",
        "\ttrain_iter,valid_iter,test_iter = data[0],data[1],data[2] # data is a tuple of three iterators\n",
        "\n",
        "\t# print(\"Start Training\")\n",
        "\tfor epoch in range(config.start_epoch,config.nepoch):\n",
        "\n",
        "\t\t## train and validation\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, train_iter, epoch,loss_fn,optimizer,config)\n",
        "\n",
        "\t\tval_loss, val_acc ,val_f1_score,val_w_f1_score,val_top3_acc= eval_model(model, valid_iter,loss_fn,config,arch_name)\n",
        "\t\tprint(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "\t\t## testing\n",
        "\t\ttest_loss, test_acc,test_f1_score,test_w_f1_score,test_top3_acc = eval_model(model, test_iter,loss_fn,config,arch_name)\n",
        "\t\tprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}% Test F1 score: {test_f1_score:.4f}')\n",
        "\n",
        "\t\t## save best model\n",
        "\t\tis_best = val_acc > best_acc1\n",
        "\t\tos.makedirs(save_home,exist_ok=True)\n",
        "\t\t# save_checkpoint({'epoch': epoch + 1,'arch': arch_name,'state_dict': model.state_dict(),'train_acc':train_acc,\"val_acc\":val_acc,'param':log_dict[\"param\"],'optimizer' : optimizer.state_dict(),},is_best,save_home+\"/checkpoint.pth.tar\")\n",
        "\n",
        "\t\tbest_acc1 = max(val_acc, best_acc1)\n",
        "\t\tif config.step_size != None:\n",
        "\t\t\tlr_scheduler.step()\n",
        "\n",
        "\t\t## tensorboard runs\n",
        "\t\twriter.add_scalar('Loss/train',train_loss,epoch)\n",
        "\t\twriter.add_scalar('Accuracy/train',train_acc,epoch)\n",
        "\t\twriter.add_scalar('Loss/val',val_loss,epoch)\n",
        "\t\twriter.add_scalar('Accuracy/val',val_acc,epoch)\n",
        "\n",
        "\t\t## save logs\n",
        "\t\tif is_best:\n",
        "\n",
        "\n",
        "\t\t\tpatience_flag = 0\n",
        "\t\t\tlog_dict[\"train_acc\"] = train_acc\n",
        "\t\t\tlog_dict[\"test_acc\"] = test_acc\n",
        "\t\t\tlog_dict[\"valid_acc\"] = val_acc\n",
        "\t\t\tlog_dict[\"test_f1_score\"] = test_f1_score\n",
        "\t\t\tlog_dict[\"valid_f1_score\"] = val_f1_score\n",
        "\t\t\tlog_dict[\"top3_acc\"] = test_top3_acc\n",
        "\t\t\tlog_dict[\"train_loss\"] = train_loss\n",
        "\t\t\tlog_dict[\"test_loss\"] = test_loss\n",
        "\t\t\tlog_dict[\"valid_loss\"] = val_loss\n",
        "\t\t\tlog_dict[\"epoch\"] = epoch+1\n",
        "\t\t\t# log_dict[\"note\"] = note\n",
        "\t\t\tlog_dict[\"weighted_test_f1_score\"] = test_w_f1_score\n",
        "\t\t\tlog_dict[\"weighted_valid_f1_score\"] = val_w_f1_score\n",
        "\n",
        "\n",
        "\t\t\twith open(save_home+\"/log.json\", 'w') as fp:\n",
        "\t\t\t\tjson.dump(log_dict, fp,indent=4)\n",
        "\t\t\tfp.close()\n",
        "\t\telse:\n",
        "\t\t\tpatience_flag += 1\n",
        "\n",
        "\t\t## early stopping\n",
        "\t\tif patience_flag == config.patience or epoch == config.nepoch-1:\n",
        "\t\t\tprint(log_dict)\n",
        "\t\t\tbreak\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t# # note = \"without dom\" ## to note any changes\n",
        "  log_dict = {}\n",
        "  log_dict[\"param\"] = train_config.param\n",
        "\n",
        "  if train_config.tuning:\n",
        "    for arch_name in [\"kea_electra\"]:\n",
        "      for lr in [3e-05]: ## for tuning\n",
        "        print('Loading dataset')\n",
        "        start_time = time.time()\n",
        "        vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(train_config.batch_size,train_config.tokenizer,train_config.dataset,arch_name)\n",
        "\n",
        "        data = (train_iter,valid_iter,test_iter)\n",
        "        finish_time = time.time()\n",
        "        print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
        "        learning_rate = lr\n",
        "        arch_name = arch_name\n",
        "        log_dict[\"param\"][\"learning_rate\"] = lr\n",
        "        log_dict[\"param\"][\"arch_name\"] = arch_name\n",
        "        note = \"Tuning learning_rate:\"+str(lr)\n",
        "\n",
        "        # learning_rate = train_config.learning_rate\n",
        "        input_type = train_config.input_type\n",
        "\n",
        "        ## Initialising model, loss, optimizer, lr_scheduler\n",
        "        model = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        total_steps = len(train_iter) * train_config.nepoch\n",
        "\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "        if train_config.step_size != None:\n",
        "          lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "        ## Filepaths for saving the model and the tensorboard runs\n",
        "        model_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "        writer = SummaryWriter('/content/gdrive/MyDrive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "        save_home = \"/content/gdrive/MyDrive/emotion_recognition/save/\"+train_config.dataset+\"/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "\n",
        "        train_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n",
        "      \n",
        "\telse:\n",
        "\t\tlearning_rate = train_config.learning_rate\n",
        "\n",
        "\t\tarch_name = train_config.arch_name\n",
        "\n",
        "\t\tinput_type = train_config.input_type\n",
        "\n",
        "\t\t## Initialising model, loss, optimizer, lr_scheduler\n",
        "\t\tmodel = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "\n",
        "\t\tif train_config.dataset != \"goemotions\":\n",
        "\t\t\tloss_fn = nn.CrossEntropyLoss()\n",
        "\t\telse:\n",
        "\t\t\tloss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\t\ttotal_steps = len(train_iter) * train_config.nepoch\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "\n",
        "\n",
        "\t\tlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\t\t## Filepaths for saving the model and the tensorboard runs\n",
        "\t\tmodel_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "\t\twriter = SummaryWriter('/content/gdrive/MyDrive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "\t\tsave_home = \"/content/gdrive/MyDrive/emotion_recognition/save/\"+train_config.dataset+\"/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "\t\ttrain_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset\n",
            "Finished loading. Time taken:01.791 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings_project.weight', 'electra.embeddings_project.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Idx: 100, Training Loss: 3.3664, Training Accuracy:  20.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 200, Training Loss: 3.1116, Training Accuracy:  10.00%, Time taken:  0.52 min\n",
            "Epoch: 01, Idx: 300, Training Loss: 3.3866, Training Accuracy:  10.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 400, Training Loss: 2.9052, Training Accuracy:  0.00%, Time taken:  0.53 min\n",
            "Epoch: 01, Idx: 500, Training Loss: 2.8122, Training Accuracy:  20.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 600, Training Loss: 2.8780, Training Accuracy:  10.00%, Time taken:  0.53 min\n",
            "Epoch: 01, Idx: 700, Training Loss: 2.7296, Training Accuracy:  30.00%, Time taken:  0.55 min\n",
            "Epoch: 01, Idx: 800, Training Loss: 2.3483, Training Accuracy:  10.00%, Time taken:  0.55 min\n",
            "Epoch: 01, Idx: 900, Training Loss: 2.6082, Training Accuracy:  20.00%, Time taken:  0.53 min\n",
            "Epoch: 01, Idx: 1000, Training Loss: 1.9112, Training Accuracy:  20.00%, Time taken:  0.57 min\n",
            "Epoch: 01, Idx: 1100, Training Loss: 2.3433, Training Accuracy:  40.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 1200, Training Loss: 1.9688, Training Accuracy:  50.00%, Time taken:  0.53 min\n",
            "Epoch: 01, Idx: 1300, Training Loss: 2.3020, Training Accuracy:  40.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 1400, Training Loss: 1.6551, Training Accuracy:  30.00%, Time taken:  0.55 min\n",
            "Epoch: 01, Idx: 1500, Training Loss: 2.2203, Training Accuracy:  10.00%, Time taken:  0.54 min\n",
            "Epoch: 01, Idx: 1600, Training Loss: 1.4512, Training Accuracy:  60.00%, Time taken:  0.53 min\n",
            "Epoch: 01, Idx: 1700, Training Loss: 1.7608, Training Accuracy:  40.00%, Time taken:  0.55 min\n",
            "Epoch: 01, Idx: 1800, Training Loss: 1.3330, Training Accuracy:  70.00%, Time taken:  0.52 min\n",
            "Epoch: 01, Idx: 1900, Training Loss: 1.8062, Training Accuracy:  60.00%, Time taken:  0.56 min\n",
            "Epoch: 01, Train Loss: 2.405, Train Acc: 29.27%, Val. Loss: 1.872805, Val. Acc: 44.30%\n",
            "Test Loss: 1.975, Test Acc: 40.29% Test F1 score: 0.3577\n",
            "Epoch: 02, Idx: 100, Training Loss: 0.9068, Training Accuracy:  60.00%, Time taken:  0.52 min\n",
            "Epoch: 02, Idx: 200, Training Loss: 1.5909, Training Accuracy:  40.00%, Time taken:  0.53 min\n",
            "Epoch: 02, Idx: 300, Training Loss: 2.0495, Training Accuracy:  30.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 400, Training Loss: 1.3871, Training Accuracy:  40.00%, Time taken:  0.53 min\n",
            "Epoch: 02, Idx: 500, Training Loss: 1.2380, Training Accuracy:  60.00%, Time taken:  0.51 min\n",
            "Epoch: 02, Idx: 600, Training Loss: 2.1312, Training Accuracy:  30.00%, Time taken:  0.52 min\n",
            "Epoch: 02, Idx: 700, Training Loss: 2.0689, Training Accuracy:  20.00%, Time taken:  0.55 min\n",
            "Epoch: 02, Idx: 800, Training Loss: 2.0125, Training Accuracy:  30.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 900, Training Loss: 0.9819, Training Accuracy:  70.00%, Time taken:  0.52 min\n",
            "Epoch: 02, Idx: 1000, Training Loss: 0.9729, Training Accuracy:  80.00%, Time taken:  0.56 min\n",
            "Epoch: 02, Idx: 1100, Training Loss: 1.9278, Training Accuracy:  40.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 1200, Training Loss: 1.5933, Training Accuracy:  50.00%, Time taken:  0.55 min\n",
            "Epoch: 02, Idx: 1300, Training Loss: 1.4843, Training Accuracy:  70.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 1400, Training Loss: 1.9406, Training Accuracy:  40.00%, Time taken:  0.52 min\n",
            "Epoch: 02, Idx: 1500, Training Loss: 1.3520, Training Accuracy:  80.00%, Time taken:  0.53 min\n",
            "Epoch: 02, Idx: 1600, Training Loss: 1.3952, Training Accuracy:  50.00%, Time taken:  0.56 min\n",
            "Epoch: 02, Idx: 1700, Training Loss: 1.2493, Training Accuracy:  30.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 1800, Training Loss: 1.8760, Training Accuracy:  40.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Idx: 1900, Training Loss: 1.2162, Training Accuracy:  50.00%, Time taken:  0.54 min\n",
            "Epoch: 02, Train Loss: 1.583, Train Acc: 51.35%, Val. Loss: 1.625571, Val. Acc: 52.82%\n",
            "Test Loss: 1.749, Test Acc: 49.38% Test F1 score: 0.4702\n",
            "Epoch: 03, Idx: 100, Training Loss: 0.9657, Training Accuracy:  50.00%, Time taken:  0.56 min\n",
            "Epoch: 03, Idx: 200, Training Loss: 1.6019, Training Accuracy:  40.00%, Time taken:  0.54 min\n",
            "Epoch: 03, Idx: 300, Training Loss: 1.4931, Training Accuracy:  50.00%, Time taken:  0.52 min\n",
            "Epoch: 03, Idx: 400, Training Loss: 0.9853, Training Accuracy:  70.00%, Time taken:  0.54 min\n",
            "Epoch: 03, Idx: 500, Training Loss: 0.4112, Training Accuracy:  90.00%, Time taken:  0.54 min\n",
            "Epoch: 03, Idx: 600, Training Loss: 0.9290, Training Accuracy:  70.00%, Time taken:  0.56 min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b4b66f055ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_home\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m#       learning_rate = train_config.learning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b4b66f055ffd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config, data, model, loss_fn, optimizer, lr_scheduler, writer, save_home)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;31m## train and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mval_f1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_w_f1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_top3_acc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0march_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b4b66f055ffd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_iter, epoch, loss_fn, optimizer, config)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m## model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# print(\"Prediction\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5nrmB-dP5-e"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "Insert the required parameters in the config file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IS68MGPXiG"
      },
      "source": [
        "# **Evaluation** \n",
        "\n",
        "Insert model path and log path to run the evaluation"
      ]
    }
  ]
}