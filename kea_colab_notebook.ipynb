{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "kea_colab_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipkiVoYv29y"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56689Kbtv4aL"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IozsiDVvwVHX"
      },
      "source": [
        "pip install transformers==3.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buPM1PEEw0M1"
      },
      "source": [
        "import sys\n",
        "sys.path.append(<insert path to project directory in drive>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5nrmB-dP5-e"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "Insert the required parameters in the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5KsX8_v1B3"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "## torch packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "## for visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## custom\n",
        "from eval import eval_model\n",
        "from select_model_input import select_model,select_input\n",
        "import dataset\n",
        "import config as train_config\n",
        "from label_dict import label_emo_map\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best,filename='checkpoint.pth.tar'):\n",
        "    torch.save(state,filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename,filename.replace('checkpoint.pth.tar','model_best.pth.tar'))\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iter, epoch,loss_fn,optimizer,config):\n",
        "    \n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    start_train_time = time.time()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "\n",
        "        text, attn, target = select_input(batch,config,arch_name)\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        print(text[0].size())\n",
        "        if (target.size()[0] is not config.batch_size):# Last batch may have length different than config.batch_size\n",
        "            continue\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "          if arch_name ==\"kea_electra\" or arch_name == \"kea_bert\":\n",
        "                text = [text[0].cuda(),text[1].cuda(),text[2].cuda(),text[3].cuda()]\n",
        "          else:\n",
        "            text = text.cuda()\n",
        "          attn = attn.cuda()\n",
        "          target = target.cuda()\n",
        "\n",
        "        ## model prediction\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Prediction\")\n",
        "        prediction = model(text,attn)\n",
        "        # print(\"computing loss\")\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        ## evaluation\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/config.batch_size\n",
        "        # print(\"Loss backward\")\n",
        "        startloss = time.time()\n",
        "        loss.backward()\n",
        "        # print(time.time()-startloss,\"Finish loss\")\n",
        "        clip_gradient(model, 1e-1)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(),1)\n",
        "        optimizer.step()\n",
        "        # print(\"=====================\")\n",
        "        steps += 1\n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1:02}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%, Time taken: {((time.time()-start_train_time)/60): .2f} min')\n",
        "            start_train_time = time.time()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def train_model(config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home):\n",
        "\n",
        "    best_acc1 = 0\n",
        "    patience_flag = 0\n",
        "    train_iter,valid_iter,test_iter = data[0],data[1],data[2] # data is a tuple of three iterators\n",
        "\n",
        "    # print(\"Start Training\")\n",
        "    for epoch in range(config.start_epoch,config.nepoch):\n",
        "\n",
        "        ## train and validation\n",
        "        train_loss, train_acc = train_epoch(model, train_iter, epoch,loss_fn,optimizer,config)\n",
        "\n",
        "        val_loss, val_acc ,val_f1_score,val_w_f1_score,val_top3_acc= eval_model(model, valid_iter,loss_fn,config,arch_name)\n",
        "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "        ## testing\n",
        "        test_loss, test_acc,test_f1_score,test_w_f1_score,test_top3_acc = eval_model(model, test_iter,loss_fn,config,arch_name)\n",
        "        print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}% Test F1 score: {test_f1_score:.4f}')\n",
        "\n",
        "        ## save best model\n",
        "        is_best = val_acc > best_acc1\n",
        "        os.makedirs(save_home,exist_ok=True)\n",
        "        save_checkpoint({'epoch': epoch + 1,'arch': arch_name,'state_dict': model.state_dict(),'train_acc':train_acc,\"val_acc\":val_acc,'param':log_dict[\"param\"],'optimizer' : optimizer.state_dict(),},is_best,save_home+\"/checkpoint.pth.tar\")\n",
        "\n",
        "        best_acc1 = max(val_acc, best_acc1)\n",
        "        if config.step_size != None:\n",
        "          lr_scheduler.step()\n",
        "\n",
        "        ## tensorboard runs\n",
        "        writer.add_scalar('Loss/train',train_loss,epoch)\n",
        "        writer.add_scalar('Accuracy/train',train_acc,epoch)\n",
        "        writer.add_scalar('Loss/val',val_loss,epoch)\n",
        "        writer.add_scalar('Accuracy/val',val_acc,epoch)\n",
        "\n",
        "        ## save logs\n",
        "        if is_best:\n",
        "\n",
        "\n",
        "            patience_flag = 0\n",
        "            log_dict[\"train_acc\"] = train_acc\n",
        "            log_dict[\"test_acc\"] = test_acc\n",
        "            log_dict[\"valid_acc\"] = val_acc\n",
        "            log_dict[\"test_f1_score\"] = test_f1_score\n",
        "            log_dict[\"valid_f1_score\"] = val_f1_score\n",
        "            log_dict[\"top3_acc\"] = test_top3_acc\n",
        "            log_dict[\"train_loss\"] = train_loss\n",
        "            log_dict[\"test_loss\"] = test_loss\n",
        "            log_dict[\"valid_loss\"] = val_loss\n",
        "            log_dict[\"epoch\"] = epoch+1\n",
        "            log_dict[\"note\"] = note\n",
        "            log_dict[\"weighted_test_f1_score\"] = test_w_f1_score\n",
        "            log_dict[\"weighted_valid_f1_score\"] = val_w_f1_score\n",
        "\n",
        "\n",
        "            with open(save_home+\"/log.json\", 'w') as fp:\n",
        "                json.dump(log_dict, fp,indent=4)\n",
        "            fp.close()\n",
        "        else:\n",
        "            patience_flag += 1\n",
        "\n",
        "        ## early stopping\n",
        "        if patience_flag == config.patience or epoch == config.nepoch-1:\n",
        "            print(log_dict)\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # note = \"without dom\" ## to note any changes\n",
        "    log_dict = {}\n",
        "    log_dict[\"param\"] = train_config.param\n",
        "    print(train_config.learning_rate)\n",
        "    print(train_config.arch_name)\n",
        "    print(train_config.batch_size)\n",
        "    ## Loading data\n",
        "    print('Loading dataset')\n",
        "    start_time = time.time()\n",
        "    vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(train_config.batch_size,train_config.tokenizer,train_config.embedding_type,train_config.arch_name)\n",
        "\n",
        "    data = (train_iter,valid_iter,test_iter)\n",
        "    finish_time = time.time()\n",
        "    print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
        "    if train_config.tuning:\n",
        "\n",
        "    ## Initialising parameters from train_config\n",
        "\n",
        "        for lr in [1e-05,2e-05,3e-05]: ## for tuning \n",
        "          learning_rate = lr\n",
        "          arch_name = train_config.arch_name\n",
        "          log_dict[\"param\"][\"learning_rate\"] = lr\n",
        "          log_dict[\"param\"][\"arch_name\"] = arch_name\n",
        "          note = \"Tuning learning_rate:\"+str(lr)\n",
        "\n",
        "          # learning_rate = train_config.learning_rate\n",
        "\n",
        "          \n",
        "\n",
        "          input_type = train_config.input_type\n",
        "\n",
        "          ## Initialising model, loss, optimizer, lr_scheduler\n",
        "          model = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "          total_steps = len(train_iter) * train_config.nepoch\n",
        "\n",
        "          optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "          if train_config.step_size != None:\n",
        "            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "          ## Filepaths for saving the model and the tensorboard runs\n",
        "          model_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "          writer = SummaryWriter('./runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "          save_home = \"./save/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "          train_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n",
        "    else:\n",
        "        learning_rate = train_config.learning_rate\n",
        "\n",
        "        arch_name = train_config.arch_name\n",
        "\n",
        "        input_type = train_config.input_type\n",
        "       \n",
        "        ## Initialising model, loss, optimizer, lr_scheduler\n",
        "        model = select_model(train_config,arch_name,vocab_size,word_embeddings)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        total_steps = len(train_iter) * train_config.nepoch\n",
        "\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
        "\n",
        "\n",
        "\n",
        "        ## Filepaths for saving the model and the tensorboard runs\n",
        "        model_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
        "        writer = SummaryWriter('/content/gdrive/My Drive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
        "        save_home = \"/content/gdrive/My Drive/emotion_recognition/save/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
        "\n",
        "\n",
        "        train_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IS68MGPXiG"
      },
      "source": [
        "# **Evaluation** \n",
        "\n",
        "Insert model path and log path to run the evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2uLQ3YIGZYt"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from easydict import EasyDict as edict\n",
        "import argparse\n",
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pickle\n",
        "import pandas as pd\n",
        "## torch packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "## for visulisation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "## custom\n",
        "from select_model_input import select_model,select_input\n",
        "import dataset\n",
        "from label_dict import emo_label_map,label_emo_map,class_names,class_indices\n",
        "\n",
        "# from xai_emo_rec import explain_model\n",
        "# from comparison import do_comparison\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "def accuracy_topk(output, target, topk=(3,)):\n",
        "    \"\"\" Taken fromhttps://discuss.pytorch.org/t/imagenet-example-accuracy-calculation/7840/2\n",
        "    Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    # print(pred,target)\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res[0].item()\n",
        "\n",
        "def get_pred_softmax(logits):\n",
        "    softmax_layer = nn.Softmax(dim=1)\n",
        "    return softmax_layer(logits)\n",
        "\n",
        "def eval_model(model, val_iter, loss_fn,config,arch_name,mode=\"train\",explain=False):\n",
        "\n",
        "    confusion = config.confusion\n",
        "    per_class = config.per_class\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    total_epoch_acc3 = 0\n",
        "\n",
        "    eval_batch_size = 1\n",
        "\n",
        "    if confusion:\n",
        "        conf_matrix = torch.zeros(config.output_size, config.output_size)\n",
        "    if per_class:\n",
        "           class_correct = list(0. for i in range(config.output_size))\n",
        "           class_total = list(0. for i in range(config.output_size))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            model = model.cuda()\n",
        "            text, attn,target = select_input(batch,config,arch_name)\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "              if arch_name ==\"kea_electra\" or arch_name == \"kea_bert\":\n",
        "                text = [text[0].cuda(),text[1].cuda(),text[2].cuda(),text[3].cuda()]\n",
        "              else:\n",
        "                text = text.cuda()\n",
        "              attn = attn.cuda()\n",
        "              target = target.cuda()\n",
        "\n",
        "            prediction = model(text,attn)\n",
        "\n",
        "            correct = np.squeeze(torch.max(prediction, 1)[1].eq(target.view_as(torch.max(prediction, 1)[1])))\n",
        "            pred_ind = torch.max(prediction, 1)[1].view(target.size()).data\n",
        "\n",
        "            if mode == \"explain\":\n",
        "                pred_softmax = get_pred_softmax(prediction)\n",
        "                explain_model(model,text,target.data,batch[\"utterance_data_str\"],pred_ind,pred_softmax) ## use jupyter-notebook while doing explainations\n",
        "            else:\n",
        "                if confusion:\n",
        "                    for t, p in zip(target.data, pred_ind):\n",
        "                            conf_matrix[t.long(), p.long()] += 1\n",
        "                if per_class:\n",
        "                    label = target[0]\n",
        "                    class_correct[label] += correct.item()\n",
        "                    class_total[label] += 1\n",
        "\n",
        "                loss = loss_fn(prediction, target)\n",
        "\n",
        "                num_corrects = (pred_ind == target.data).sum()\n",
        "                y_true.extend(target.data.cpu().tolist())\n",
        "                y_pred.extend(pred_ind.cpu().tolist())\n",
        "\n",
        "                acc = 100.0 * num_corrects/eval_batch_size\n",
        "                acc3 = accuracy_topk(prediction, target, topk=(3,))\n",
        "                total_epoch_loss += loss.item()\n",
        "                total_epoch_acc += acc.item()\n",
        "                total_epoch_acc3 += acc3\n",
        "\n",
        "        if confusion:\n",
        "            import seaborn as sns\n",
        "            plt.figure(figsize=(30,30))\n",
        "            sns.heatmap(conf_matrix, annot=True,xticklabels=list(emo_label_map.keys()),yticklabels=list(emo_label_map.keys()),cmap='Blues')\n",
        "            plt.show()\n",
        "        if per_class:\n",
        "            for i in range(config.output_size):\n",
        "                print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                label_emo_map[i], 100 * class_correct[i] / class_total[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "            label_per_class, acc_per_class = [],[]\n",
        "            for i in range(config.output_size):\n",
        "                label_per_class.append(label_emo_map[i])\n",
        "                acc_per_class.append(100 * class_correct[i] / class_total[i])\n",
        "\n",
        "      \n",
        "    report = classification_report(y_true,y_pred,target_names=list(emo_label_map.keys()),output_dict=True)\n",
        "    f1_per_class = []\n",
        "    for label in label_per_class:\n",
        "      f1_per_class.append(report[label][\"f1-score\"])\n",
        "    df_per_class = pd.DataFrame({\"emotion class\":label_per_class, \"accuracy\":acc_per_class,\"f1 score\":f1_per_class})\n",
        "    df_per_class.to_csv(\"/content/gdrive/My Drive/emotion_recognition/result_log/\"+arch_name+\"_per_class.csv\")\n",
        "    f1_score_e = f1_score(y_true, y_pred, labels=class_indices,average='macro')\n",
        "    f1_score_w = f1_score(y_true, y_pred, labels=class_indices,average='weighted')\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter),f1_score_e,f1_score_w,total_epoch_acc3/len(val_iter),conf_matrix\n",
        "\n",
        "\n",
        "\n",
        "def load_model(resume,model,optimizer):\n",
        "\n",
        "\n",
        "    checkpoint = torch.load(resume)\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "    optimizer.load_state_dict(checkpoint['optimizer']) ## during retrain TODO\n",
        "\n",
        "    return model,optimizer,start_epoch\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    resume_path = \"\" ## insert model path\n",
        "    log_path = \"\" ## insert log path\n",
        "    mode = \"eval\"\n",
        "    rem_epoch = 10 # only for retraining TODO\n",
        "    patience = \"\"\n",
        "    comp_resume_path = \"\" # for comparison \n",
        "    ## Load the resume model parameters\n",
        "   \n",
        "    with open(log_path,'r') as f:\n",
        "        log = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "    if mode == \"compare\":\n",
        "        do_comparison(resume_path,comp_resume_path)\n",
        "        exit()\n",
        "\n",
        "    ## Initialising parameters\n",
        "    learning_rate = log[\"param\"][\"learning_rate\"]\n",
        "    batch_size = 1 ## batch_size=1 for testing and validation\n",
        "    input_type = log[\"param\"][\"input_type\"]\n",
        "    arch_name = log[\"param\"][\"arch_name\"]\n",
        "    hidden_size = log[\"param\"][\"hidden_size\"]\n",
        "    embedding_length = log[\"param\"][\"embedding_length\"]\n",
        "    output_size = log[\"param\"][\"output_size\"]\n",
        "    tokenizer = log[\"param\"][\"tokenizer\"]\n",
        "    embedding_type = log[\"param\"][\"embedding_type\"]\n",
        "\n",
        "    ## Loading data\n",
        "    print('Loading dataset')\n",
        "    start_time = time.time()\n",
        "    vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(batch_size,tokenizer,embedding_type,arch_name)\n",
        "    finish_time = time.time()\n",
        "    print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
        "\n",
        "    eval_config = edict(log[\"param\"])\n",
        "    eval_config.param = log[\"param\"]\n",
        "    eval_config.resume_path = resume_path\n",
        "    eval_config.batch_size = 1  ## batch_size=1 for testing and validation\n",
        "\n",
        "    if mode == \"explain\":\n",
        "        model = select_model(eval_config,vocab_size,word_embeddings,grad_check=False)\n",
        "    else:\n",
        "        model = select_model(eval_config,arch_name,vocab_size,word_embeddings)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,eval_config.step_size, gamma=0.5)\n",
        "\n",
        "    model,optimizer,start_epoch = load_model(resume_path,model,optimizer)\n",
        "\n",
        "\n",
        "    print(f'Train Acc: {log[\"train_acc\"]:.3f}%, Valid Acc: {log[\"valid_acc\"]:.3f}%, Test Acc: {log[\"test_acc\"]:.3f}%')\n",
        "\n",
        "    eval_config.confusion = True\n",
        "    eval_config.per_class = True\n",
        "\n",
        "    ## testing\n",
        "\n",
        "    test_loss, test_acc,test_f1_score,test_f1_score_w,test_top3_acc,conf_matrix= eval_model(model, test_iter,loss_fn,eval_config,arch_name,mode)\n",
        "    print(test_acc)\n",
        "    print(f'top-1 acc: {test_acc:.3f}%, top-3 acc: {test_top3_acc:.3f}%, F1 Score: {test_f1_score:.3f}, F1 Score W: {test_f1_score_w:.3f}')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}