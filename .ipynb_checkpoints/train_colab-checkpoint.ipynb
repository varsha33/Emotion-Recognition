{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import random \n",
    "import time\n",
    "import argparse\n",
    "## torch packages\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "\n",
    "## for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## custom\n",
    "from eval import eval_model\n",
    "from select_model_input import select_model,select_input\n",
    "import dataset\n",
    "import config as train_config\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best,filename='checkpoint.pth.tar'):\n",
    "    torch.save(state,filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename,filename.replace('checkpoint.pth.tar','model_best.pth.tar'))\n",
    "\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, train_iter, epoch,loss_fn,optimizer,config):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    start_train_time = time.time()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "\n",
    "        text, target = select_input(batch,config)\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        \n",
    "        if (target.size()[0] is not config.batch_size):# One of the batch returned has length different than config.batch_size\n",
    "            continue\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            if config.arch_name == \"sl_bert\" or config.arch_name == \"a_bert\":\n",
    "                text = [text[0].cuda(),text[1].cuda()]\n",
    "            else:\n",
    "                text = text.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        ## model prediction\n",
    "        model.zero_grad()  \n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        ## evaluation\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/config.batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1\n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1:02}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%, Time taken: {((time.time()-start_train_time)/60): .2f} min')\n",
    "            start_train_time = time.time()\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def train_model(config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home):\n",
    "    \n",
    "    best_acc1 = 0\n",
    "    patience_flag = 0\n",
    "    train_iter,valid_iter,test_iter = data[0],data[1],data[2] # data is a tuple of three iterators\n",
    "    log_dict = {}\n",
    "    log_dict[\"param\"] = config.param\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(config.start_epoch,config.nepoch):\n",
    "    \n",
    "        ## train and validation\n",
    "        train_loss, train_acc = train_epoch(model, train_iter, epoch,loss_fn,optimizer,config)\n",
    "        val_loss, val_acc ,val_f1_score= eval_model(model, valid_iter,loss_fn,config)\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        ## testing\n",
    "        test_loss, test_acc,test_f1_score = eval_model(model, test_iter,loss_fn,config)\n",
    "        print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        ## save best model\n",
    "        is_best = test_acc > best_acc1\n",
    "        os.makedirs(save_home,exist_ok=True)\n",
    "        save_checkpoint({'epoch': epoch + 1,'arch': config.arch_name,'state_dict': model.state_dict(),'test_acc': test_acc,'train_acc':train_acc,\"val_acc\":val_acc,'param':log_dict[\"param\"],'optimizer' : optimizer.state_dict(),},is_best,save_home+\"/checkpoint.pth.tar\")\n",
    "        best_acc1 = max(test_acc, best_acc1)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        ## tensorboard runs\n",
    "        writer.add_scalar('Loss/train',train_loss,epoch)\n",
    "        writer.add_scalar('Accuracy/train',train_acc,epoch)\n",
    "        writer.add_scalar('Loss/val',val_loss,epoch)\n",
    "        writer.add_scalar('Accuracy/val',val_acc,epoch)\n",
    "\n",
    "        ## save logs\n",
    "        if is_best:\n",
    "            patience_flag = 0\n",
    "            log_dict[\"train_acc\"] = train_acc\n",
    "            log_dict[\"test_acc\"] = test_acc\n",
    "            log_dict[\"valid_acc\"] = val_acc\n",
    "            log_dict[\"test_f1_score\"] = test_f1_score\n",
    "            log_dict[\"valid_f1_score\"] = val_f1_score\n",
    "            log_dict[\"train_loss\"] = train_loss\n",
    "            log_dict[\"test_loss\"] = test_loss\n",
    "            log_dict[\"valid_loss\"] = val_loss\n",
    "            log_dict[\"epoch\"] = epoch+1\n",
    "            log_dict[\"note\"] = note\n",
    "            \n",
    "            with open(save_home+\"/log.json\", 'w') as fp:\n",
    "                json.dump(log_dict, fp,indent=4)\n",
    "            fp.close()\n",
    "        else:\n",
    "            patience_flag += 1\n",
    "\n",
    "        ## early stopping\n",
    "        if patience_flag == config.patience or epoch == config.nepoch-1:\n",
    "            print(log_dict)\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Anything to note specific for this run')\n",
    "\n",
    "    parser.add_argument('-n',type=str,default=\"\",help='Anything to note specific for this run')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    note = args.n\n",
    "    ## Loading data\n",
    "    print('Loading dataset')\n",
    "    start_time = time.time()\n",
    "    vocab_size, word_embeddings,train_iter, valid_iter ,test_iter= dataset.get_dataloader(train_config.batch_size,train_config.tokenizer,train_config.embedding_type,train_config.arch_name)\n",
    "    data = (train_iter,valid_iter,test_iter)\n",
    "    finish_time = time.time()\n",
    "    print('Finished loading. Time taken:{:06.3f} sec'.format(finish_time-start_time))\n",
    "\n",
    "\n",
    "    ## Initialising parameters from train_config\n",
    "    learning_rate = train_config.learning_rate\n",
    "    arch_name = train_config.arch_name\n",
    "    input_type = train_config.input_type\n",
    "\n",
    "    ## Initialising model, loss, optimizer, lr_scheduler\n",
    "    model = select_model(train_config,vocab_size,word_embeddings)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,train_config.step_size, gamma=0.5)\n",
    "    \n",
    "\n",
    "    ## Filepaths for saving the model and the tensorboard runs\n",
    "    model_run_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "    writer = SummaryWriter('/content/gdrive/My Drive/emotion_recognition/runs/'+input_type+\"/\"+arch_name+\"/\")\n",
    "    save_home = \"'/content/gdrive/My Drive/emotion_recognition/save/\"+input_type+\"/\"+arch_name+\"/\"+model_run_time\n",
    "    \n",
    "    train_model(train_config,data,model,loss_fn,optimizer,lr_scheduler,writer,save_home)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
